{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import minari\n",
    "import d3rlpy\n",
    "from d3rlpy.datasets import MDPDataset\n",
    "\n",
    "from d3rlpy.algos import CQLConfig, BEARConfig, AWACConfig, BCQConfig\n",
    "from d3rlpy.metrics.evaluators import EnvironmentEvaluator\n",
    "\n",
    "import pickle as pk\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Wrapper for certain environments with obs['observatioon']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnvWrapper(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        obs, info = self.env.reset(**kwargs)\n",
    "        return obs[\"observation\"], info\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "        return obs[\"observation\"], reward, terminated, truncated, info\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load dataset from Minari"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset = minari.load_dataset(\"mujoco/inverteddoublependulum/expert-v0\", download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert and save dataset in d3rlpy format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m2025-04-18 15:14.13\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mSignatures have been automatically determined.\u001b[0m \u001b[36maction_signature\u001b[0m=\u001b[35mSignature(dtype=[dtype('float32')], shape=[(1,)])\u001b[0m \u001b[36mobservation_signature\u001b[0m=\u001b[35mSignature(dtype=[dtype('float64')], shape=[(9,)])\u001b[0m \u001b[36mreward_signature\u001b[0m=\u001b[35mSignature(dtype=[dtype('float64')], shape=[(1,)])\u001b[0m\n",
      "\u001b[2m2025-04-18 15:14.13\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mAction-space has been automatically determined.\u001b[0m \u001b[36maction_space\u001b[0m=\u001b[35m<ActionSpace.CONTINUOUS: 1>\u001b[0m\n",
      "\u001b[2m2025-04-18 15:14.13\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mAction size has been automatically determined.\u001b[0m \u001b[36maction_size\u001b[0m=\u001b[35m1\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Convert Minari to D3RLpy Dataset Format\n",
    "observations = np.concatenate([ep.observations[:-1] for ep in dataset.iterate_episodes()])\n",
    "# next_observations = np.concatenate([ep.observations[1:] for ep in dataset.iterate_episodes()])\n",
    "actions = np.concatenate([ep.actions for ep in dataset.iterate_episodes()])\n",
    "rewards = np.concatenate([ep.rewards for ep in dataset.iterate_episodes()])\n",
    "terminals = np.concatenate([ep.terminations for ep in dataset.iterate_episodes()])\n",
    "timeouts = np.concatenate([ep.truncations for ep in dataset.iterate_episodes()])\n",
    "\n",
    "mdp_dataset = MDPDataset(observations, actions, rewards, terminals, timeouts)\n",
    "\n",
    "# save the modified dataset\n",
    "with open('doublependulum_expert_v0.pkl', 'wb') as f:\n",
    "    pk.dump(mdp_dataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 100000, 100000, 100000, 100000)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(observations), len(actions), len(rewards), len(terminals), len(timeouts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load converted dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the modified dataset\n",
    "with open('pointmaze_umaze_dense_v2.pkl', 'rb') as f:\n",
    "    dataset = pk.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define environment (if required)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = gym.make('InvertedPendulum-v5')\n",
    "# Create the wrapped environment\n",
    "# wrapped_env = EnvWrapper(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = d3rlpy.load_learnable('d3rlpy_logs/bear_hopper_simple_v0_batch_1024_steps_100000_20250416203710/model_40000.d3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Reward = 66.85510679398618\n",
      "Episode 2: Reward = 112.65848230983802\n",
      "Episode 3: Reward = 104.13716182496448\n",
      "Episode 4: Reward = 90.42100719906452\n",
      "Episode 5: Reward = 171.83542111522274\n",
      "Episode 6: Reward = 137.08998799975765\n",
      "Episode 7: Reward = 129.84512177452945\n",
      "Episode 8: Reward = 90.72568261717194\n",
      "Episode 9: Reward = 118.70784036963096\n",
      "Episode 10: Reward = 78.89335858457541\n"
     ]
    }
   ],
   "source": [
    "# Evaluation loop\n",
    "n_episodes = 10\n",
    "\n",
    "for episode in range(n_episodes):\n",
    "    obs, _ = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0.0\n",
    "\n",
    "    while not done:\n",
    "        action = model.predict(np.expand_dims(obs, axis=0))[0]  # predict takes a batch\n",
    "        obs, reward, terminated, truncated, _ = env.step(action)\n",
    "        # print(reward)\n",
    "        total_reward += reward\n",
    "        done = terminated or truncated\n",
    "\n",
    "    print(f\"Episode {episode + 1}: Reward = {total_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cql = CQLConfig(batch_size=1024).create(device='cuda:0')\n",
    "# cql = CQLConfig().create(device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cql.fit(\n",
    "    dataset=dataset,\n",
    "    n_steps = 3000,           # total number of gradient updates\n",
    "    n_steps_per_epoch = 100,  # epochs total\n",
    "    save_interval = 50,           # save every 1 epoch\n",
    "    experiment_name=\"cql_pointmaze_umaze_dense_v2_batch_1024\",\n",
    "    with_timestamp=True,\n",
    "    show_progress=True,\n",
    "    evaluators={\n",
    "        \"environment\": EnvironmentEvaluator(wrapped_env),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BEAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "bear = BEARConfig(batch_size=1024).create(device='cuda:0')\n",
    "# cql = CQLConfig().create(device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bear.fit(\n",
    "    dataset=dataset,\n",
    "    n_steps = 3000,           # total number of gradient updates\n",
    "    n_steps_per_epoch = 1000,  # 3 epochs total\n",
    "    save_interval = 1,           # save every 1 epoch\n",
    "    experiment_name=\"bear_pointmaze_umaze_dense_v2_batch_1024\",\n",
    "    with_timestamp=True,\n",
    "    show_progress=True,\n",
    "    evaluators={\n",
    "        \"environment\": EnvironmentEvaluator(wrapped_env),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AWAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "awac = AWACConfig(batch_size=1024).create(device='cuda:0')\n",
    "# cql = CQLConfig().create(device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "awac.fit(\n",
    "    dataset=dataset,\n",
    "    n_steps = 3000,           # total number of gradient updates\n",
    "    n_steps_per_epoch = 1000,  # 3 epochs total\n",
    "    save_interval = 1,           # save every 1 epoch\n",
    "    experiment_name=\"awac_pointmaze_umaze_dense_v2_batch_1024\",\n",
    "    with_timestamp=True,\n",
    "    show_progress=True,\n",
    "    evaluators={\n",
    "        \"environment\": EnvironmentEvaluator(wrapped_env),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BCQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "bcq = BCQConfig(batch_size=1024).create(device='cuda:0')\n",
    "# cql = CQLConfig().create(device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bcq.fit(\n",
    "    dataset=dataset,\n",
    "    n_steps = 3000,           # total number of gradient updates\n",
    "    n_steps_per_epoch = 1000,  # 3 epochs total\n",
    "    save_interval = 1,           # save every 1 epoch\n",
    "    experiment_name=\"bcq_pointmaze_umaze_dense_v2_batch_1024\",\n",
    "    with_timestamp=True,\n",
    "    show_progress=True,\n",
    "    evaluators={\n",
    "        \"environment\": EnvironmentEvaluator(wrapped_env),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
